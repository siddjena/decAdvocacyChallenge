{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Your Python Environment\n",
        "\n",
        "**To use a different virtual environment for this notebook:**\n",
        "\n",
        "1. Press `Ctrl+Shift+P` to open the command palette\n",
        "2. Type \"Python: Select Interpreter\"\n",
        "3. Choose your venv's Python executable (usually in `venv\\Scripts\\python.exe`)\n",
        "\n",
        "The notebook will automatically use the selected interpreter. No need to register the venv as a Jupyter kernel separately.\n",
        "\n",
        "**Note:** Make sure your selected environment has all required packages installed, including `ipykernel` (see the cell below). The `ipykernel` package is required for Jupyter notebooks to work with your Python environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X1rOkoU6NAQI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipykernel in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (7.1.0)\n",
            "Requirement already satisfied: daft in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (0.6.14)\n",
            "Requirement already satisfied: xarray in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (2025.11.0)\n",
            "Requirement already satisfied: numpy in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (2.3.4)\n",
            "Requirement already satisfied: pandas in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: matplotlib in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (3.10.7)\n",
            "Requirement already satisfied: comm>=0.1.1 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (0.2.3)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (1.8.17)\n",
            "Requirement already satisfied: ipython>=7.23.1 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (9.7.0)\n",
            "Requirement already satisfied: jupyter-client>=8.0.0 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (8.6.3)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (5.9.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.4 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging>=22 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (25.0)\n",
            "Requirement already satisfied: psutil>=5.7 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (7.1.3)\n",
            "Requirement already satisfied: pyzmq>=25 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (27.1.0)\n",
            "Requirement already satisfied: tornado>=6.2 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (6.5.2)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipykernel) (5.14.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from daft) (22.0.0)\n",
            "Requirement already satisfied: fsspec in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from daft) (2025.10.0)\n",
            "Requirement already satisfied: tqdm in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from daft) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from matplotlib) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: colorama>=0.4.4 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
            "Requirement already satisfied: decorator>=4.3.2 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.18.1 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.11.0 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
            "Requirement already satisfied: stack_data>=0.6.0 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
            "Requirement already satisfied: wcwidth in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel) (0.8.5)\n",
            "Requirement already satisfied: platformdirs>=2.5 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in d:\\decisionanalytics-visualization\\environment\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (0.2.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install all required packages for this notebook\n",
        "# Only run this cell if packages are missing or need updating\n",
        "# If using a virtual environment, make sure it's activated and selected as your Python interpreter first\n",
        "\n",
        "%pip install ipykernel daft xarray numpy pandas matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCEzzpvpM4os"
      },
      "source": [
        "# Multi-Dimensional Arrays for Decision Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "AR1rIaFVM4ov",
        "results": "hide"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'daft' has no attribute 'PGM'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m default_rng\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdag\u001b[39;00m(\u001b[43mdaft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPGM\u001b[49m):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     14\u001b[39m         daft.PGM.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
            "\u001b[31mAttributeError\u001b[39m: module 'daft' has no attribute 'PGM'"
          ]
        }
      ],
      "source": [
        "#@title A Full Decision Model\n",
        "#| echo: false\n",
        "#| include: false\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from functools import partial, partialmethod\n",
        "import daft   ### %pip install -U git+https://github.com/daft-dev/daft.git\n",
        "from numpy.random import default_rng\n",
        "import numpy as np\n",
        "\n",
        "class dag(daft.PGM):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        daft.PGM.__init__(self, *args, **kwargs)\n",
        "\n",
        "    obsNode = partialmethod(daft.PGM.add_node, aspect = 2.2, fontsize = 10, plot_params = {'facecolor': 'cadetblue'})\n",
        "    decNode = partialmethod(daft.PGM.add_node, aspect = 2.2, fontsize = 10, shape = \"rectangle\", plot_params = {'facecolor': 'thistle'})\n",
        "    detNode = partialmethod(daft.PGM.add_node, aspect = 5.4, fontsize = 9.25, alternate = True, plot_params = {'facecolor': 'aliceblue'})\n",
        "    latNode = partialmethod(daft.PGM.add_node, scale = 1.2, aspect = 2.2, fontsize = 10, plot_params = {'facecolor': 'aliceblue'})\n",
        "    detNodeBig = partialmethod(daft.PGM.add_node, scale = 1.6, aspect = 2.25, fontsize = 10, alternate = True, plot_params = {'facecolor': 'aliceblue'})\n",
        "    latNodeBig = partialmethod(daft.PGM.add_node, scale = 1.6, aspect = 2.2, fontsize = 10, plot_params = {'facecolor': 'aliceblue'})\n",
        "\n",
        "pgm = dag(dpi = 300, alternate_style=\"outer\")\n",
        "pgm.latNode(\"d\",\"Demand\\n\"+r\"$(d_i)$\",1,2)\n",
        "pgm.decNode(\"q\",\"Order\\n\" + r\"Qty$(q)$\",6,2)\n",
        "pgm.detNode(\"pi\",\"Profit: \" + r\"$\\pi(d_i,q) =$\" + \"\\n\" + r\"$3 \\times \\min(d_i,q) - 1 \\times q$\", 3.5,2)\n",
        "pgm.detNode(\"l\",\"Lost Sales\\n\" + r\"$\\ell(d_i,q) = \\max(0,d_i-q)$\", 3.5,1)\n",
        "pgm.add_edge(\"d\",\"pi\")\n",
        "pgm.add_edge(\"q\",\"pi\")\n",
        "pgm.add_edge(\"d\",\"l\")\n",
        "pgm.add_edge(\"q\",\"l\")\n",
        "pgm.add_plate([0.25, 0.25, 5, 2.25], label = \"Sim Num:\\n\" + r\"$i = 1, 2, 3$\",\n",
        "              label_offset = (2,2), rect_params = dict({\"fill\": False, \"linestyle\": \"dashed\", \"edgecolor\": \"cadetblue\"}))\n",
        "pgm.add_plate([2, -0.1, 4.7, 2.75], label = \"Order Quantity:\\n\" + r\"$q = 30, 40, 50$\",\n",
        "              label_offset = (2,2), position = \"bottom right\", rect_params = dict({\"fill\": False, \"linestyle\": \"dotted\", \"edgecolor\": \"darkorchid\"}))\n",
        "pgm.show(dpi = 140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idsZ4GOFM4ox"
      },
      "source": [
        "Around the nodes, notice the new addition of rectangles,technically called _plates_.  \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "i &\\equiv \\textrm{Index for simulation draws. } i \\in \\{1, 2, 3\\}\\\\\n",
        "d_i &\\equiv \\textrm{Daily demand for newspapers, and}\\\\\n",
        "d_i &\\sim \\textrm{Binomial}(n=200,p=0.2).\\\\\n",
        "q &\\equiv \\textrm{Order quantity chosen by decision-maker, where}\\\\\n",
        "q &\\in \\{30, 40, 50\\} \\qquad \\textit{   (potentially good order qtys.)}.\\\\\n",
        "\\pi &\\equiv \\textrm{Daily profit is revenue minus expenses.}\\\\\n",
        "\\pi(d_i,q) &= 3 \\times \\min(d_i,q) - 1 \\times q \\qquad \\textit{   (cannot sell more than ordered)}.\\\\\n",
        "\\ell &\\equiv \\textrm{Lost sales.  Unmet demand due to being out of stock.}\\\\\n",
        "\\ell(d_i,q) &= \\max(0, d-q) \\qquad \\textit{   (lost sales cannot be negative)}.\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPjDkVVhM4oy"
      },
      "outputs": [],
      "source": [
        "potentialCusts = 200\n",
        "purchaseProb = 0.2\n",
        "\n",
        "rng = default_rng(seed = 111)\n",
        "numSims = 3\n",
        "\n",
        "# create data frame to store simulated demand\n",
        "newsDF = pd.DataFrame({\"simNum\": range(1, numSims+1),  # sequence of 1 to 100\n",
        "                   \"demand\": rng.binomial(n = potentialCusts,\n",
        "                                          p = purchaseProb,\n",
        "                                          size = numSims)})\n",
        "\n",
        "## google SEARCH PHRASE: get element-wise minimum of two columns in pandas dataframe\n",
        "newsDF[\"profit_q40\"] = 3 * np.minimum(newsDF.demand,40) - 1 * 40\n",
        "newsDF[\"lostSales_q40\"] = np.maximum(0,newsDF.demand - 40)\n",
        "\n",
        "# view first few 5 rows of newsDF\n",
        "newsDF.iloc[:5,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTktsxDbM4oy"
      },
      "source": [
        "It will feel cumbersome to add more columns for each order quantity's profit and each quantity's lost sales into a dataframe.  There must be a better way to structure how we store this data.  \n",
        "\n",
        "## The `xarray` Package\n",
        "\n",
        "### `DataArray`:  In its simplest 1-dimensional form, a `DataArray` is just a collection of values, like the column of dataframe (`pandas.Series`) or a one-dimensional array of values (`numpy.ndarray`).  We can create a simple `DataArray` using its constructor function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zXM9o4iM4oz"
      },
      "outputs": [],
      "source": [
        "from numpy.random import default_rng\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "rng = default_rng(seed = 111)  ## set random seed\n",
        "demand = rng.binomial(n=200,p=0.2,size=3)   ## get demand values\n",
        "\n",
        "## make data array\n",
        "xr.DataArray(data = demand)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmmBLUOvM4oz"
      },
      "source": [
        "\n",
        "*   dimension key-value pair `dim_0: 3` tells us that the cardinality of our\n",
        "demand array is 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KEQLtJeM4oz"
      },
      "outputs": [],
      "source": [
        "## make data array with labelled dimension name\n",
        "xr.DataArray(data = demand, dims = \"draw\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn23rMM8M4o0"
      },
      "outputs": [],
      "source": [
        "## explicit labeling of coordinates - must use name now to create dataset later\n",
        "demandDA = xr.DataArray(data = demand, coords = {\"draw\": np.arange(3)+1}, name = \"demand\")\n",
        "demandDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbv7S3rpM4o0"
      },
      "source": [
        "Notice, we can drop the `dims` arguments as the dimension name is supplied in the dictionary object passed to the `coords` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Nsd7VnhYP5Pq"
      },
      "outputs": [],
      "source": [
        "#@title Repeating the graphical model\n",
        "pgm.show(dpi=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9iRMJNHM4o1"
      },
      "outputs": [],
      "source": [
        "## creating a DataArray of order quantities - must use name now to create dataset later\n",
        "orderDA = xr.DataArray(data = [30, 40, 50],\n",
        "                       coords = {\"orderQtyIndex\": [30,40,50]},\n",
        "                       name = \"orderQty\")\n",
        "orderDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO6jEIVXQMuf"
      },
      "source": [
        "## Merge the two data arrays into a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUPPqJXcM4o2"
      },
      "outputs": [],
      "source": [
        "# create dataset by combining data arrays\n",
        "newsvDS = xr.merge([demandDA,orderDA])\n",
        "newsvDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ4IkEz4M4o2"
      },
      "source": [
        "Our `Dataset` container now has the dimensions implied by the plate indices in @fig-newsvGM2, namely `draw: 3  orderQty: 3`.  In math terms we have two sets, one is a set of demand draws where cardinality $|D|=3$; the other a set of order quantities with cardinality $|Q|=3$.  Thus, the set of all ordered pairs $(d_i,q)$ to be used for calculation of profit and lost sales will have cardinality $|D| \\times |Q| = 9$.  Thus, our simulation of potential  $\\pi(d_i,q)$ and $\\ell(d_i,q)$ values will each have 9 elements.\n",
        "\n",
        "## Seven mental models of dataset manipulation\n",
        "\n",
        "The seven most important ways we might want to manipulate a data array or dataset is to:\n",
        "\n",
        "1. Assign: `.assign()` or `assign_coords()`: Add data variables with broadcasting and array math.  (Can also use dict-like methods)\n",
        "2. Subset: `.sel()` or `.where()` subset a data array or dataset based on coordinates or data values, respectively.\n",
        "3. Drop: `.drop_vars()` or `.drop_dims()`: Remove an explicit list of data variables or remove all data variables indexed by a particular dimension.\n",
        "4. Sort: `.sortby()` sorts or arranges a data array or dataset based on data values or coordinate values.\n",
        "5. Aggregate: See the [xarray documentation](https://docs.xarray.dev/en/stable/api.html?highlight=aggregation#) for a list of aggregation functions.  These functions will collapse all the data of a given dimension; for example one can collapse a time dimension using the `mean()` aggregation method to get the average value for all of time.\n",
        "6. Split-Apply-Combine: `.groupby()` and `DatasetGroupBy.foo()` are usually used in combination to 1) _split_ the dataset into groups based on levels of a variable, 2) _apply_ a function (e.g. `foo()`) to each group's dataset individually, and then 3) _combine_ the modified datasets. See the [xarray documentation](https://docs.xarray.dev/en/stable/api.html?highlight=Groupby#groupby-objects) for more details.\n",
        "7. Merge(join): Getting information from two datasets to intelligently combine.\n",
        "\n",
        "### 1 - Assign: Adding Data Arrays\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JCe1hUUM4o2"
      },
      "outputs": [],
      "source": [
        "(  ## open parenthesis to start readable code\n",
        "    newsvDS\n",
        "    .assign(soldNewspapers = np.minimum(newsvDS.demand,newsvDS.orderQty))\n",
        ") ## close parenthesis finishes the \"method chaining\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlve-DMbM4o3"
      },
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "(  ## open parenthesis to start readable code\n",
        "    newsvDS\n",
        "    .assign(soldNewspapers = np.minimum(newsvDS.demand,newsvDS.orderQty))\n",
        "    .assign(revenue = 3 * newsvDS.soldNewspapers)\n",
        ") ## close parenthesis finishes the \"method chaining\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F48oW-VyM4o3"
      },
      "source": [
        "The above code will yield an error:\n",
        "\n",
        "```\n",
        "AttributeError: 'Dataset' object has no attribute 'soldNewspapers'\n",
        "```\n",
        "\n",
        "The last assignment apparently does not have visibility into the newly created data for `soldNewspapers`.  To pass the _current state_ of the dataset to the `.assign()` method, we use a `lambda` function.  The `lambda` function has syntax `lambda arguments : expression` where `lambda` is a keyword telling python to expect an argument (or arguments), followed by a colon (`:`), and then an expression for what will be returned by the function..  Here is updated code that works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHj3_ps7M4o3"
      },
      "outputs": [],
      "source": [
        "(  ## open parenthesis to start readable code\n",
        "    newsvDS\n",
        "    .assign(soldNewspapers = np.minimum(newsvDS.demand,newsvDS.orderQty))\n",
        "    .assign(revenue = lambda DS: 3 * DS.soldNewspapers)\n",
        ") ## use lambda function to get current state of dataset in chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZLOuenEM4o3"
      },
      "source": [
        "## YOUR TURN:  Use `assign` to add a lost sales data variable to the dataset.  Modify the below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqCRsj7OM4o4"
      },
      "outputs": [],
      "source": [
        "newsvDS = (newsvDS\n",
        "            .assign(soldNewspapers = np.minimum(newsvDS.demand,newsvDS.orderQty))\n",
        "            .assign(revenue = lambda DS: 3 * DS.soldNewspapers)\n",
        "            .assign(expense = 1 * newsvDS.orderQty)\n",
        "            .assign(profit = lambda DS: DS.revenue - DS.expense)\n",
        ")\n",
        "\n",
        "(newsvDS\n",
        " .to_dataframe())  #dataframe for printing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVMJq5fdM4o4"
      },
      "source": [
        "Note, one can also add columns directly using dict-like indexing when chains of operations are not required.  The following code would work similarly to what we did earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUD4jUUnM4o4"
      },
      "outputs": [],
      "source": [
        "newsvDS[\"lostSales\"] = np.maximum(0, newsvDS.demand - newsvDS.orderQty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Lg50UFM4o4"
      },
      "source": [
        "### 2 - Select a subset of the data array or dataset\n",
        "\n",
        "Syntax of help documentation is often `packagename.Class.method`.  Let's find the `sel` method in the xarray documentation. (https://docs.xarray.dev/en/stable/user-guide/indexing.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc874LIrM4o4"
      },
      "outputs": [],
      "source": [
        "# select a particular value for a dimension\n",
        "newsvDS.sel(orderQtyIndex = 30) # returns 1-d dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zew0qgxxM4o5"
      },
      "source": [
        "`xarray` uses the `slice()` function to select ranges of coordinate values, following the pandas convention. **Important:** Unlike standard Python slicing (which excludes the end point), `slice()` in xarray is **inclusive of both start and end points**. For example, `slice(36, 58)` includes both 36 and 58 in the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmNoP3kxM4o5"
      },
      "outputs": [],
      "source": [
        "# slicing returns all values inside the range (inclusive)\n",
        "# as long as the index labels are monotonic increasing\n",
        "newsvDS.sel(orderQtyIndex = slice(36,58))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rKlevrjM4o5"
      },
      "source": [
        "Slicing returns a smaller dataset or data array based on coordinates, but often we want a smaller dataset based on data values.  In these cases, we apply the `.where()` method where the argument is some logical condition for which data to keep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeU29la4M4o5"
      },
      "outputs": [],
      "source": [
        "# need to explicitly use DataSet.DataArray syntax for\n",
        "# filtering out rows that do not meet condition\n",
        "newsvDS.assign(lostSales = lambda DS: DS.revenue - DS.expense).where(newsvDS.lostSales > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4S1S4tAM4o5"
      },
      "source": [
        "Often times, the `lambda` syntax for anonymous functions gets used to pass in the dataset name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yr71C5wM4o5"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    newsvDS.where(lambda x: x.lostSales > 0, drop = True)\n",
        "    .to_dataframe()  #convert to pandas dataframe for printing\n",
        "    .dropna() # pandas method to remove NaN rows\n",
        " )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EXA0H3kM4o6"
      },
      "source": [
        "## YOUR TURN:  \n",
        "Experiment with omitting the `pandas.DataFrame.dropna` method from the above.  What's different.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG3IFMkiTSx1"
      },
      "outputs": [],
      "source": [
        "## experiment here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM5UvVIqTVeh"
      },
      "source": [
        "\n",
        "### 3 - Drop Dimensions\n",
        "\n",
        "The `drop_dims()` method returns a new object by dropping a full dimension from a dataset along with any variables whose coordinates rely on that dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEQo8oFKM4o6"
      },
      "outputs": [],
      "source": [
        "newsvDS.drop_dims(\"orderQtyIndex\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr-Le6IgM4o6"
      },
      "source": [
        "Above, the order quantity dimension is dropped along with all the data variables whose value depended on order quantity: `orderQty`, `soldNewspapers`, `revenue`, `expense`, `profit`, and `lostSales`.\n",
        "\n",
        "If you want to just drop some of the data variables, you use `drop_vars()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xql27qCM4o6"
      },
      "outputs": [],
      "source": [
        "newsvDS.drop_vars([\"revenue\",\"expense\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHGo92eVM4o6"
      },
      "source": [
        "### 4 - Sort a data array or dataset based on data values or data values.\n",
        "\n",
        "We will typically want dataframe-like reports generated out of `xarray` as a last step in data manipulation.  We will rely on `pandas.DataFrame.sort_values()` to help us for this mental model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weUjUhrkM4o7"
      },
      "outputs": [],
      "source": [
        "(newsvDS\n",
        " .to_dataframe()\n",
        " .sort_values(\"profit\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ifVlPpTnqn"
      },
      "source": [
        "## YOUR TURN:\n",
        "Using `.sort_values(\"profit\", ascending = True)` reverse the sort order so maximum profit is first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocwywxJvT3uK"
      },
      "outputs": [],
      "source": [
        "# Experiment Here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouUD_Wn4M4o7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### 5 - Aggregation\n",
        "\n",
        "See the `xarray` documentation at https://docs.xarray.dev/en/stable/api.html#id6 for a complete list of aggregation functions.\n",
        "\n",
        "1) Aggregate the information in a data array.\n",
        "2) Assign the output of the aggregation to a new data array in a pre-existing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbb5dlv2M4o7"
      },
      "outputs": [],
      "source": [
        "## collapse the 100 draws into 1 summary statistic\n",
        "(\n",
        "    newsvDS\n",
        "    .profit\n",
        "    .mean(dim = \"draw\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAghVf-_M4o7"
      },
      "source": [
        "Notice, this returns a `DataArray` object.  We will then keep our data and summary statistics together in one dataset by adding the array back to the original dataset using `assign()`.  Here the two-step workflow is demonstrated to return expected profit and expected lost sales for each order quantity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hr8Bw1iM4o7"
      },
      "outputs": [],
      "source": [
        "## create mean summary stats\n",
        "(\n",
        "    newsvDS\n",
        "    .assign(expProfit = newsvDS.profit.mean(dim=\"draw\"))\n",
        "    .assign(expLossSales = newsvDS.lostSales.mean(dim=\"draw\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKHZn1_1M4o8"
      },
      "source": [
        "Feel free to play around with these other frequently-used aggregation functions include `count`, `first`, `last`, `max`, `mean`, `median`, `min`, `quantile`, and `sum.`\n",
        "\n",
        "### 6 - Split-Apply-Combine\n",
        "\n",
        "See the [xarray documentation](https://docs.xarray.dev/en/stable/api.html?highlight=Groupby#groupby-objects) for more details using split-apply-combine:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpFQv0uwV_y3"
      },
      "outputs": [],
      "source": [
        "## find average profit by orderQty\n",
        "## see docuemntation here: https://docs.xarray.dev/en/stable/generated/xarray.core.groupby.DatasetGroupBy.mean.html\n",
        "(\n",
        "    newsvDS\n",
        "    .get(\"profit\")\n",
        "    .groupby(\"orderQtyIndex\")\n",
        "    .mean(...)\n",
        ").to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94KfqeldWZ1p"
      },
      "source": [
        "## YOUR TURN\n",
        "Find the maximum lost sales by order quantity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCYIjZ2mWe3W"
      },
      "outputs": [],
      "source": [
        "# Code Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdFKsMIKV83X"
      },
      "source": [
        "### 7 - Merge(join):\n",
        "\n",
        "The `merge()` function combines two or more datasets by aligning them along shared dimensions and coordinates. This is useful when you have related data in separate datasets that you want to combine into one.\n",
        "\n",
        "**Example:** Suppose we have supplier cost information in a separate dataset and want to merge it with our existing newspaper vendor dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataset with supplier cost information per order quantity\n",
        "supplierDS = xr.Dataset({\n",
        "    \"unitCost\": ([\"orderQtyIndex\"], [0.95, 0.90, 0.85]),  # bulk discount: larger orders get lower unit cost\n",
        "    \"shippingCost\": ([\"orderQtyIndex\"], [5, 8, 10])  # shipping increases with order size\n",
        "}, coords={\"orderQtyIndex\": [30, 40, 50]})\n",
        "\n",
        "supplierDS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge the supplier dataset with the existing newsvDS dataset\n",
        "# xarray automatically aligns on shared coordinates (orderQtyIndex in this case)\n",
        "combinedDS = xr.merge([newsvDS, supplierDS])\n",
        "\n",
        "combinedDS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can use the merged data to calculate profit with the new cost structure\n",
        "# For example, recalculate profit using the supplier's unit cost instead of the fixed $1 cost\n",
        "(\n",
        "    combinedDS\n",
        "    .assign(adjustedExpense = lambda DS: DS.unitCost * DS.orderQty + DS.shippingCost)\n",
        "    .assign(adjustedProfit = lambda DS: DS.revenue - DS.adjustedExpense)\n",
        "    .to_dataframe()\n",
        "    .loc[:, [\"orderQtyIndex\", \"draw\", \"profit\", \"adjustedProfit\"]]\n",
        "    .head(10)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key points about `merge()`:**\n",
        "\n",
        "- `merge()` automatically aligns datasets on shared coordinates (in this case, `orderQtyIndex`)\n",
        "- Variables from both datasets are combined into a single dataset\n",
        "- If datasets have different dimensions, `merge()` will broadcast appropriately\n",
        "- The merged dataset contains all variables from all input datasets\n",
        "\n",
        "This is particularly useful when combining data from different sources or when adding new variables to an existing analysis.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Environment",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
